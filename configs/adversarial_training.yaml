# Iterative Adversarial Training Configuration

# Training parameters (inherited from base training config)
training:
  base_model: "models/baseline/checkpoint_epoch_3.pt"  # Use existing trained model
  dataset: "promoter"  # Dataset to use
  max_length: 300
  batch_size: 16
  # Use full dataset (remove sample limits for full experiment)
  learning_rate: 0.000005  # Smaller LR for fine-tuning
  num_epochs: 3  # Full epochs per iteration
  fine_tune: true  # Enable fine-tuning mode
  warmup_steps: 500  # Full warmup steps
  num_classes: 2
  weight_decay: 0.01
  seed: 42

# Adversarial training parameters
adversarial_training:
  max_iterations: 5  # Maximum number of adversarial training iterations
  adversarial_ratio: 0.2  # Ratio of adversarial examples to original data
  save_adversarial_examples: true
  adversarial_data_dir: "data/adversarial/iterative_training"
  start_from_iteration: 1  # Skip iteration 0, start from existing model
  convergence_threshold: 0.001  # Stop early if improvement < 0.1%

# Model checkpointing
checkpointing:
  save_dir: "models/adversarial_training"
  save_best_only: true
  monitor_metric: "val_accuracy"
  save_frequency: 1  # Save after each iteration

# Monitoring and plotting
monitoring:
  plot_accuracy: true
  plot_dir: "plots/adversarial_training"
  save_metrics: true
  metrics_file: "data/adversarial/iterative_training/training_metrics.csv"
  log_level: "INFO"

# Attack configuration (inherits from genetic attack)
attack_config: "configs/attack_genetic.yaml"

# Output configuration
output:
  results_dir: "results/adversarial_training"
  save_final_model: true
  generate_report: true
