# Iterative Adversarial Training Configuration

# Training parameters (inherited from base training config)
training:
  base_model: "zhihan1996/DNABERT-2-117M"  # Start from pretrained model
  dataset: "promoter"  # Dataset to use
  max_length: 300
  batch_size: 32  # Larger batch for speed
  # Use minimal dataset for fast testing
  train_samples: 100  # Only 100 training samples
  val_samples: 50     # Only 50 validation samples
  eval_samples: 50    # Only 50 evaluation samples
  learning_rate: 0.00002  # Standard LR for full retraining
  num_epochs: 1  # Only 1 epoch for speed
  fine_tune: false  # Disable fine-tuning - use full retraining
  warmup_steps: 10  # Minimal warmup for speed
  num_classes: 2
  weight_decay: 0.01
  seed: 42

# Adversarial training parameters
adversarial_training:
  max_iterations: 3  # Maximum number of adversarial training iterations
  adversarial_ratio: 0.2  # Ratio of adversarial examples to original data
  save_adversarial_examples: true
  adversarial_data_dir: "data/adversarial/iterative_training"
  start_from_iteration: 0  # Start from scratch, retrain baseline
  convergence_threshold: 0.001  # Stop early if improvement < 0.1%

# Model checkpointing
checkpointing:
  save_dir: "models/adversarial_training"
  save_best_only: true
  monitor_metric: "val_accuracy"
  save_frequency: 1  # Save after each iteration

# Monitoring and plotting
monitoring:
  plot_accuracy: true
  plot_dir: "plots/adversarial_training"
  save_metrics: true
  metrics_file: "data/adversarial/iterative_training/training_metrics.csv"
  log_level: "INFO"

# Attack configuration (inherits from genetic attack)
attack_config: "configs/attack_genetic.yaml"

# Output configuration
output:
  results_dir: "results/adversarial_training"
  save_final_model: true
  generate_report: true
